{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft of feature matching\n",
    "\n",
    "Adapted from: https://github.com/anerv/BikeDNA_BIG/blob/main/feature_matching_hpc/scripts/feature_matching.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries, settings and data\n",
    "\n",
    "debug = False\n",
    "\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import pickle\n",
    "\n",
    "import momepy as mm\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "import contextily as cx\n",
    "\n",
    "import utils\n",
    "import matching_functions as match_func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user settings for feature matching algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature matching user settings\n",
    "segment_length = 10  # The shorter the segments, the longer the matching process will take. For cities with a gridded street network with streets as straight lines, longer segments will usually work fine\n",
    "buffer_dist = 15\n",
    "hausdorff_threshold = 17\n",
    "angular_threshold = 30\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user settings for use case (FUA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which FUA?\n",
    "fua = 869\n",
    "\n",
    "# which h3 resolution?\n",
    "res = 9\n",
    "\n",
    "base_crs = \"EPSG:4326\"\n",
    "\n",
    "# baseinal OSM data\n",
    "orig_file = f\"../data/{fua}/roads_osm.parquet\"\n",
    "\n",
    "# which file to take as baseline?\n",
    "base_file = f\"../data/{fua}/manual/{fua}.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = utils.read_sample_data()\n",
    "geom = meta.loc[meta.eFUA_ID == fua, \"geometry\"]\n",
    "city = meta.loc[meta.eFUA_ID == fua, \"eFUA_name\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in orig OSM data\n",
    "orig = utils.read_parquet_roads(fua)\n",
    "orig = orig[[\"geometry\"]]\n",
    "\n",
    "# get projected CRS \n",
    "proj_crs = orig.crs\n",
    "\n",
    "# read in base case (manual simp)\n",
    "base = utils.read_manual(fua, proj_crs=orig.crs)\n",
    "\n",
    "# which file to compare with?\n",
    "comp = utils.read_parenx(fua, \"voronoi\", proj_crs=orig.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get graphs & node/edge gdfs for both data sets\n",
    "base_graph = mm.gdf_to_nx(base, length=\"length\", integer_labels=True)\n",
    "comp_graph = mm.gdf_to_nx(comp, length=\"length\", integer_labels=True)\n",
    "nodes_base, edges_base = mm.nx_to_gdf(base_graph)\n",
    "nodes_comp, edges_comp = mm.nx_to_gdf(comp_graph)\n",
    "# add node degree to nodes df\n",
    "nodes_base = utils.add_node_degree(nodes_base, base_graph)\n",
    "nodes_comp = utils.add_node_degree(nodes_comp, comp_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make h3 grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = utils.make_grid(fua, res, proj_crs=proj_crs)\n",
    "grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make subfolder for results, and save grid there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"../data/{fua}/evaluation/\", exist_ok=True)\n",
    "grid.to_file(f\"../data/{fua}/evaluation/grid.gpkg\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define parameters\n",
    "\n",
    "# study_area = parsed_yaml_file[\"study_area\"] study_area: 'dk' # provide name of study area\n",
    "# study_crs = parsed_yaml_file[\"study_crs\"] study_crs: 'EPSG:25832' # The CRS you want to use for the analysis. \n",
    "# reference_name = parsed_yaml_file[\"reference_name\"] reference_name: 'GeoDanmark' # provide name of reference dataset\n",
    "\n",
    "# # define filepaths and read in data\n",
    "\n",
    "# path = f\"../{study_area}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and plot grid\n",
    "\n",
    "def create_h3_grid(polygon_gdf, hex_resolution, crs, buffer_dist):\n",
    "\n",
    "    # Inspired by https://stackoverflow.com/questions/51159241/how-to-generate-shapefiles-for-h3-hexagons-in-a-particular-area\n",
    "\n",
    "    print(f\"Creating hexagons at resolution {hex_resolution}...\")\n",
    "\n",
    "    union_poly = polygon_gdf.buffer(buffer_dist).to_crs(\"EPSG:4326\").geometry.unary_union\n",
    "\n",
    "    # Find the hexagons within the shape boundary using PolyFill\n",
    "    hex_list=[]\n",
    "    for n,g in enumerate(union_poly):\n",
    "        temp = mapping(g)\n",
    "        temp['coordinates']=[[[j[1],j[0]] for j in i] for i in temp['coordinates']]  \n",
    "        hex_list.extend(h3.polyfill(temp,res=hex_resolution))\n",
    "\n",
    "    # Create hexagon data frame\n",
    "    hex_pd = pd.DataFrame(hex_list,columns=[\"hex_id\"])\n",
    "\n",
    "    # Create hexagon geometry and GeoDataFrame\n",
    "    hex_pd['geometry'] = [Polygon(h3.h3_to_geo_boundary(x, geo_json=True)) for x in hex_pd[\"hex_id\"]]\n",
    "\n",
    "    grid = gpd.GeoDataFrame(hex_pd)\n",
    "\n",
    "    grid.set_crs(\"4326\",inplace=True).to_crs(crs, inplace=True)\n",
    "\n",
    "    grid[\"grid_id\"] = grid.hex_id\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "\n",
    "set_renderer(renderer_map)\n",
    "\n",
    "grid = eval_func.create_h3_grid(study_area_poly, 8, study_crs, 500)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=pdict[\"fsmap\"])\n",
    "grid_cell_area = grid.area.mean() / 1000000 # in km2\n",
    "print(f\"The grid contains {len(grid)} hex cells with an average area of {grid_cell_area:.2f} km2.\")\n",
    "print(\"This grid will be used for local (grid cell level) analysis:\")\n",
    "grid.plot(ax=ax, facecolor=\"none\", edgecolor=pdict[\"base\"], alpha=pdict[\"alpha_back\"],linewidth=0.2)\n",
    "ax.set_axis_off()\n",
    "cx.add_basemap(ax, crs=study_area_poly.crs, source=cx_tile_1)\n",
    "ax.set_title(\n",
    "    f\"{area_name} study area ({len(grid)} grid cells)\"\n",
    ")\n",
    "\n",
    "# plot_func.save_fig(fig, osm_results_static_maps_fp + \"area_grid_osm\")\n",
    "osm_processed_fp = f\"../../data/OSM/{study_area}/processed/\"\n",
    "osm_grid_fp = osm_processed_fp + \"grid.parquet\"\n",
    "\n",
    "grid.to_parquet(osm_grid_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in edges for ref and base case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_edges_simplified = gpd.read_parquet(path+\"/data/ref_edges_simplified.parquet\")\n",
    "osm_edges_simplified = gpd.read_parquet(path+\"/data/osm_edges_simplified.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read in grid for both (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_grid = gpd.read_parquet(path+\"/data/ref_grid.parquet\")\n",
    "osm_grid = gpd.read_parquet(path+\"/data/osm_grid.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = pd.merge(left=osm_grid, right=ref_grid.drop('geometry',axis=1), left_index=True, right_index=True, suffixes=('_osm','_ref'))\n",
    "assert len(grid) == len(osm_grid) == len(ref_grid)\n",
    "grid['grid_id'] = grid.grid_id_osm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for s in [segment_length, buffer_dist, hausdorff_threshold, angular_threshold]:\n",
    "    assert isinstance(s, int) or isinstance(s, float), print(\n",
    "        \"Settings must be integer or float values!\"\n",
    "    )\n",
    "\n",
    "osm_segments = match_func.create_segment_gdf(\n",
    "    osm_edges_simplified, segment_length=segment_length\n",
    ")\n",
    "osm_segments.rename(columns={\"osmid\": \"org_osmid\"}, inplace=True)\n",
    "osm_segments[\"osmid\"] = osm_segments[\n",
    "    \"edge_id\"\n",
    "]  # Because matching function assumes an id column names osmid as unique id for edges\n",
    "\n",
    "osm_segments.set_crs(study_crs, inplace=True)\n",
    "osm_segments.dropna(subset=[\"geometry\"], inplace=True)\n",
    "\n",
    "ref_segments = match_func.create_segment_gdf(\n",
    "    ref_edges_simplified, segment_length=segment_length\n",
    ")\n",
    "ref_segments.set_crs(study_crs, inplace=True)\n",
    "ref_segments.rename(columns={\"seg_id\": \"seg_id_ref\"}, inplace=True)\n",
    "ref_segments.dropna(subset=[\"geometry\"], inplace=True)\n",
    "\n",
    "print('Segments created!')\n",
    "\n",
    "osm_segments.to_parquet(path+f\"/processed/osm_segments_{segment_length}.parquet\")\n",
    "ref_segments.to_parquet(path+f\"/processed/ref_segments_{segment_length}.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_matches = match_func.overlay_buffer(\n",
    "    reference_data=ref_segments,\n",
    "    osm_data=osm_segments,\n",
    "    ref_id_col=\"seg_id_ref\",\n",
    "    osm_id_col=\"seg_id\",\n",
    "    dist=buffer_dist,\n",
    ")\n",
    "\n",
    "print('Buffer matches found!')\n",
    "\n",
    "buffer_matches.to_parquet(path+f\"/results/buffer_matches_{buffer_dist}_{hausdorff_threshold}_{angular_threshold}.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final matches\n",
    "segment_matches = match_func.find_matches_from_buffer(\n",
    "    buffer_matches=buffer_matches,\n",
    "    osm_edges=osm_segments,\n",
    "    reference_data=ref_segments,\n",
    "    angular_threshold=angular_threshold,\n",
    "    hausdorff_threshold=hausdorff_threshold,\n",
    ")\n",
    "\n",
    "print(\"Segment matches found!\")\n",
    "\n",
    "matches_fp = path+f\"/results/segment_matches_{buffer_dist}_{hausdorff_threshold}_{angular_threshold}.pickle\"\n",
    "\n",
    "with open(matches_fp, \"wb\") as f:\n",
    "    pickle.dump(segment_matches, f)\n",
    "\n",
    "print(\"Segments matching completed, results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
