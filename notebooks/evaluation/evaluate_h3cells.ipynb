{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft of H3 evaluation\n",
    "grid-cell wise comparison of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T02:22:37.567974Z",
     "iopub.status.busy": "2024-06-17T02:22:37.567620Z",
     "iopub.status.idle": "2024-06-17T02:22:37.595426Z",
     "shell.execute_reply": "2024-06-17T02:22:37.595150Z",
     "shell.execute_reply.started": "2024-06-17T02:22:37.567948Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T02:22:37.596470Z",
     "iopub.status.busy": "2024-06-17T02:22:37.596163Z",
     "iopub.status.idle": "2024-06-17T02:22:39.003166Z",
     "shell.execute_reply": "2024-06-17T02:22:39.002915Z",
     "shell.execute_reply.started": "2024-06-17T02:22:37.596456Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import folium\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import momepy as mm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "from core import eval, stats, utils, viz\n",
    "\n",
    "%watermark -w\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which variables to evaluate\n",
    "eval_vars = [\n",
    "    \"edge_count\",\n",
    "    \"edge_length\",\n",
    "    \"node_count\",\n",
    "    \"avg_degree\",\n",
    "    \"stroke_count\",\n",
    "    \"stroke_length_sum\",\n",
    "    \"stroke_length_max\",\n",
    "]\n",
    "\n",
    "# which methods to evaluate\n",
    "methods_to_evaluate = [\n",
    "    \"cityseer\",\n",
    "    \"manual\",\n",
    "    \"orig\",\n",
    "    \"osmnx\",\n",
    "    # \"parenx-voronoi\",\n",
    "    # \"parenx-skeletonize\",\n",
    "    \"sgeop\",\n",
    "]\n",
    "\n",
    "# which method pairs to compare\n",
    "methodpairs_to_compare = [\n",
    "    (\"cityseer\", \"manual\"),\n",
    "    (\"orig\", \"manual\"),\n",
    "    (\"osmnx\", \"manual\"),\n",
    "    # (\"parenx-voronoi\", \"manual\"),\n",
    "    # (\"parenx-skeletonize\", \"manual\"),\n",
    "    (\"sgeop\", \"manual\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make directories for evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalfolder = \"../../evaluation/\"\n",
    "os.makedirs(evalfolder, exist_ok=True)\n",
    "subfolders = [evalfolder + str(fua) for fua in utils.fua_city]\n",
    "for sub in subfolders:\n",
    "    os.makedirs(sub, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose use case (FUA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T02:22:39.003906Z",
     "iopub.status.busy": "2024-06-17T02:22:39.003652Z",
     "iopub.status.idle": "2024-06-17T02:22:39.005414Z",
     "shell.execute_reply": "2024-06-17T02:22:39.005224Z",
     "shell.execute_reply.started": "2024-06-17T02:22:39.003896Z"
    }
   },
   "outputs": [],
   "source": [
    "# which FUA?\n",
    "fua = 869\n",
    "\n",
    "# which h3 resolution?\n",
    "res = 9\n",
    "\n",
    "meta = utils.read_sample_data()\n",
    "geom = meta.loc[meta.eFUA_ID == fua, \"geometry\"]\n",
    "city = meta.loc[meta.eFUA_ID == fua, \"eFUA_name\"].values[0]\n",
    "\n",
    "gdf_orig = utils.read_original(fua)\n",
    "proj_crs = gdf_orig.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_grid = utils.make_grid(fua, res, proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in results from different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results from all methods into dict\n",
    "\n",
    "methods = {}\n",
    "\n",
    "for method in methods_to_evaluate:\n",
    "    print(f\"Reading in results for {method}\")\n",
    "    gdf = eval.read_method_outputs(fua, method, proj_crs)\n",
    "    gdf = gdf[~gdf.normalize().duplicated()].copy().reset_index(drop=True)\n",
    "    G = mm.gdf_to_nx(gdf, length=\"length\", integer_labels=True)\n",
    "    nodes, edges = mm.nx_to_gdf(G)\n",
    "\n",
    "    # add node degrees\n",
    "    nodes = stats.add_node_degree(nodes, G)\n",
    "\n",
    "    # add stroke IDs\n",
    "    coins = mm.COINS(edges, angle_threshold=120, flow_mode=True)\n",
    "    edges[\"stroke_id\"] = coins.stroke_attribute()\n",
    "    stroke_gdf = coins.stroke_gdf()\n",
    "\n",
    "    methods[method] = {}\n",
    "    methods[method][\"gdf\"] = gdf\n",
    "    methods[method][\"graph\"] = G\n",
    "    methods[method][\"nodes\"] = nodes\n",
    "    methods[method][\"edges\"] = edges\n",
    "\n",
    "    ### grid with stats eval for this method only\n",
    "    grid = base_grid.copy()\n",
    "\n",
    "    # add ratio columns to grid\n",
    "    grid[[\"edge_count\", \"edge_length\"]] = grid.apply(\n",
    "        lambda x: stats.get_edge_stats(edges, x.geometry),  # noqa: B023\n",
    "        axis=1,\n",
    "        result_type=\"expand\",\n",
    "    )\n",
    "    grid[[\"node_count\", \"node_degrees\", \"avg_degree\"]] = grid.apply(\n",
    "        lambda x: stats.get_node_stats(nodes, x.geometry),  # noqa: B023\n",
    "        axis=1,\n",
    "        result_type=\"expand\",\n",
    "    )\n",
    "\n",
    "    grid[[\"stroke_count\", \"stroke_length_sum\", \"stroke_length_max\"]] = grid.apply(\n",
    "        lambda x: stats.get_stroke_stats(edges, stroke_gdf, x.geometry),  # noqa: B023\n",
    "        axis=1,\n",
    "        result_type=\"expand\",\n",
    "    )\n",
    "\n",
    "    # save grid to dict\n",
    "    methods[method][\"grid\"] = grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get weights from orig-manual difference rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get \"deltas\": grid gdf of absolute differences orig-manual for weighting\n",
    "\n",
    "assert \"manual\" in methods, \"Need to read in manual data first\"\n",
    "assert \"orig\" in methods, \"Need to read in orig data first\"\n",
    "\n",
    "deltas = base_grid.copy()\n",
    "quantiles = np.arange(0, 1.1, 0.1)  # 10% steps\n",
    "\n",
    "for var in eval_vars:\n",
    "    deltas[f\"{var}_delta\"] = abs(\n",
    "        methods[\"orig\"][\"grid\"][var] - methods[\"manual\"][\"grid\"][var]\n",
    "    )\n",
    "    deltas[deltas == np.Inf] = np.NaN\n",
    "    rank = list(deltas.sort_values(by=f\"{var}_delta\", ascending=False).index)\n",
    "    # rank 0 == largest delta; rank N == smallest delta\n",
    "    rank = np.argsort(rank)\n",
    "    deltas[f\"{var}_rank\"] = rank\n",
    "\n",
    "\n",
    "colnames = [f\"{var}_rank\" for var in eval_vars]\n",
    "deltas[\"total_rank\"] = deltas[colnames].sum(axis=1)\n",
    "\n",
    "deltas[\"rank_quantile\"] = pd.qcut(\n",
    "    deltas[\"total_rank\"],\n",
    "    q=quantiles,\n",
    "    retbins=False,\n",
    "    labels=False,  # we want integer labels of ranks\n",
    ")\n",
    "\n",
    "deltas[\"rank_weight\"] = deltas.rank_quantile.apply(lambda x: 10 - x)\n",
    "\n",
    "# delete helper columns\n",
    "deltas = deltas.drop(columns=colnames + [\"rank_quantile\", \"total_rank\"])\n",
    "\n",
    "deltas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "explore weighting, does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = deltas.explore(\n",
    "    tiles=\"cartodb positron\", name=\"cells\", column=\"rank_weight\", cmap=\"Reds\"\n",
    ")\n",
    "methods[\"orig\"][\"gdf\"].explore(m=m, name=\"orig\", color=\"black\")\n",
    "methods[\"manual\"][\"gdf\"].explore(m=m, name=\"manual\", color=\"red\")\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get eval dict\n",
    "\n",
    "evaldict = {}\n",
    "\n",
    "\n",
    "for m1, m2 in methodpairs_to_compare:\n",
    "    # make sure both methods are read into dict\n",
    "    assert m1 in methods, f\"Need to read in {m1} results first!\"\n",
    "    assert m2 in methods, f\"Need to read in {m2} results first!\"\n",
    "\n",
    "    # get base grid\n",
    "    grid = base_grid.copy()\n",
    "\n",
    "    # add ratio columns\n",
    "    for var in eval_vars:\n",
    "        grid[f\"{var}_ratio\"] = methods[m1][\"grid\"][var] / methods[m2][\"grid\"][var]\n",
    "        grid[f\"{var}_delta\"] = abs(methods[m1][\"grid\"][var] - methods[m2][\"grid\"][var])\n",
    "\n",
    "    # save grid as dict entry\n",
    "    evaldict[f\"{m1}_{m2}\"] = grid\n",
    "\n",
    "    del grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "**pick up here**\n",
    "\n",
    "* pass 10-rank `np.repeat` or similar for weighting values for KDE\n",
    "* make KDE plots for each evaluation variable, for ratio and abs values,\n",
    "    * non-weighted\n",
    "    * weighted (by repeating with rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.repeat(x=evaldict[methodpair].varcolumn, a=deltas.rank_weight)\n",
    "\n",
    "# # KDE\n",
    "# npoints = len(data) // 10\n",
    "# kde = gaussian_kde(data, bw_method=\"silverman\")\n",
    "# mylinspace = np.linspace(data.min(), data.max(), npoints)\n",
    "# pdf = kde.pdf(mylinspace)\n",
    "\n",
    "# # HIST\n",
    "# plt.hist(data, nbins=50)\n",
    "\n",
    "# # with seaborn: https://seaborn.pydata.org/generated/seaborn.displot.html\n",
    "# # # sns.displot(kde=True) plots both KDE and hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var = eval_vars[6]\n",
    "# fig, ax = plt.subplots(1,1, figsize = (10,10))\n",
    "\n",
    "# for i, (m1, m2) in enumerate(methodpairs_to_compare[1:]):\n",
    "#     data = np.array(\n",
    "#             evaldict[f\"{m1}_{m2}\"][f\"{var}_delta\"]\n",
    "#         )\n",
    "#     ax.hist(\n",
    "#         data,\n",
    "#         bins=10,\n",
    "#         # alpha=0.2,\n",
    "#         label=m1,\n",
    "#         # edgecolor = \"black\",\n",
    "#         histtype=\"step\"\n",
    "#     )\n",
    "# ax.set_yscale(\"log\")\n",
    "# plt.suptitle(f\"Distribution of {var} abs deltas in method-manual\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parameter for pdf fitting\n",
    "# npoints = len(weights[\"grid\"])//10\n",
    "# print(f\"npoints: {npoints}\");\n",
    "# # get data for current option, using log of data\n",
    "# data = np.array(\n",
    "#     weights[\"grid\"][\"stroke_length_sum_delta\"]\n",
    "# )\n",
    "\n",
    "# # fit Gaussian KDE\n",
    "# kde = gaussian_kde(data, bw_method=\"silverman\")\n",
    "\n",
    "# # define linear space\n",
    "# mylinspace = np.linspace(data.min(), data.max(), npoints)\n",
    "\n",
    "# # generate probability density function\n",
    "# pdf = kde.pdf(mylinspace)\n",
    "\n",
    "# fig, ax = plt.subplots(1,1)\n",
    "\n",
    "# ax.plot(pdf)\n",
    "# #ax.hist(data, bins=npoints)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make evaldict of comparisons\n",
    "\n",
    "evaldict = {}\n",
    "\n",
    "for m1, m2 in methodpairs_to_compare:\n",
    "    # make sure both methods are read into dict\n",
    "    assert m1 in methods, f\"Need to read in {m1} results first!\"\n",
    "    assert m2 in methods, f\"Need to read in {m2} results first!\"\n",
    "\n",
    "    # get base grid\n",
    "    grid = base_grid.copy()\n",
    "\n",
    "    # add ratio columns for all evaluation variables\n",
    "    for var in eval_vars:\n",
    "        grid[f\"{var}_ratio\"] = methods[m1][\"grid\"][var] / methods[m2][\"grid\"][var]\n",
    "        grid[grid == np.Inf] = np.NaN\n",
    "\n",
    "    # save grid as dict entry\n",
    "    evaldict[f\"{m1}_{m2}\"] = grid\n",
    "\n",
    "    del grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Usage of evaldict\n",
    "\n",
    "In evaldict, evaluation results are stored.\n",
    "\n",
    "key: pair of methods (left, right); \n",
    "\n",
    "value: grid with `_ratio` columns for evaluation variables (left / right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = \"manual_parenx-voronoi\"\n",
    "var = \"stroke_length_max_ratio\"\n",
    "c1, c2 = comp.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = evaldict[comp].copy()\n",
    "m = cells.explore(tiles=\"cartodb positron\", column=var, cmap=\"Reds\", name=var)\n",
    "methods[c1][\"gdf\"].explore(m=m, name=c1, color=\"black\")\n",
    "methods[c2][\"gdf\"].explore(m=m, name=c2, color=\"green\")\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldict[\"orig_manual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldict[\"manual_orig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.PiYG\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "i = 0\n",
    "evaldict[c1].plot(\n",
    "    ax=ax[i],\n",
    "    column=var,\n",
    "    # norm=colors.CenteredNorm(vcenter=1),\n",
    "    vmin=-3,\n",
    "    vmax=5,\n",
    "    cmap=cmap,\n",
    "    legend=True,\n",
    ")\n",
    "ax[i].set_title(c1)\n",
    "\n",
    "i = 1\n",
    "evaldict[c2].plot(\n",
    "    ax=ax[i],\n",
    "    column=var,\n",
    "    # norm=colors.CenteredNorm(vcenter=1),\n",
    "    vmin=-3,\n",
    "    vmax=5,\n",
    "    cmap=cmap,\n",
    "    legend=True,\n",
    ")\n",
    "ax[i].set_title(c2)\n",
    "\n",
    "plt.suptitle(var);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = \"manual_parenx-skeletonize\"\n",
    "c2 = \"manual_sgeop\"\n",
    "var = \"edge_length_ratio\"\n",
    "\n",
    "cmap = cm.PiYG\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "i = 0\n",
    "evaldict[c1].plot(\n",
    "    ax=ax[i],\n",
    "    column=var,\n",
    "    norm=colors.CenteredNorm(vcenter=1),\n",
    "    cmap=cmap,\n",
    "    legend=True,\n",
    ")\n",
    "ax[i].set_title(c1)\n",
    "\n",
    "i = 1\n",
    "evaldict[c2].plot(\n",
    "    ax=ax[i],\n",
    "    column=var,\n",
    "    norm=colors.CenteredNorm(vcenter=1),\n",
    "    cmap=cmap,\n",
    "    legend=True,\n",
    ")\n",
    "ax[i].set_title(c2)\n",
    "\n",
    "plt.suptitle(var);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
