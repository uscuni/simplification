{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft of H3 evaluation\n",
    "grid-cell wise comparison of metrics\n",
    "* `base`: the ground truth (manually simplified)\n",
    "* `comp`: the method to compare (for now, as example: the parenx/voronoi simplification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T02:22:37.567974Z",
     "iopub.status.busy": "2024-06-17T02:22:37.567620Z",
     "iopub.status.idle": "2024-06-17T02:22:37.595426Z",
     "shell.execute_reply": "2024-06-17T02:22:37.595150Z",
     "shell.execute_reply.started": "2024-06-17T02:22:37.567948Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T02:22:37.596470Z",
     "iopub.status.busy": "2024-06-17T02:22:37.596163Z",
     "iopub.status.idle": "2024-06-17T02:22:39.003166Z",
     "shell.execute_reply": "2024-06-17T02:22:39.002915Z",
     "shell.execute_reply.started": "2024-06-17T02:22:37.596456Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "\n",
    "import folium\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import momepy as mm\n",
    "import numpy as np\n",
    "\n",
    "from core import eval, stats, utils, viz\n",
    "\n",
    "%watermark -w\n",
    "%watermark -iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which variables to evaluate\n",
    "eval_vars = [\n",
    "    \"edge_count\",\n",
    "    \"edge_length\",\n",
    "    \"node_count\",\n",
    "    \"avg_degree\",\n",
    "    \"stroke_count\",\n",
    "    \"stroke_length_sum\",\n",
    "    \"stroke_length_max\",\n",
    "]\n",
    "\n",
    "# which methods to evaluate\n",
    "methods_to_evaluate = [\n",
    "    \"orig\",\n",
    "    \"manual\",\n",
    "    \"osmnx\",\n",
    "    # \"parenx-voronoi\",\n",
    "    # \"parenx-skeletonize\",\n",
    "    \"sgeop\",\n",
    "]\n",
    "\n",
    "# which method pairs to compare\n",
    "methodpairs_to_compare = [\n",
    "    (\"orig\", \"manual\"),\n",
    "    # (\"osmnx\", \"manual\"),\n",
    "    # (\"parenx-voronoi\", \"manual\"),\n",
    "    # (\"parenx-skeletonize\", \"manual\"),\n",
    "    # (\"sgeop\", \"manual\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make directories for evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalfolder = \"../../evaluation/\"\n",
    "os.makedirs(evalfolder, exist_ok=True)\n",
    "subfolders = [evalfolder + str(fua) for fua in utils.fua_city]\n",
    "for sub in subfolders:\n",
    "    os.makedirs(sub, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose use case (FUA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T02:22:39.003906Z",
     "iopub.status.busy": "2024-06-17T02:22:39.003652Z",
     "iopub.status.idle": "2024-06-17T02:22:39.005414Z",
     "shell.execute_reply": "2024-06-17T02:22:39.005224Z",
     "shell.execute_reply.started": "2024-06-17T02:22:39.003896Z"
    }
   },
   "outputs": [],
   "source": [
    "# which FUA?\n",
    "fua = 869\n",
    "\n",
    "# which h3 resolution?\n",
    "res = 9\n",
    "\n",
    "meta = utils.read_sample_data()\n",
    "geom = meta.loc[meta.eFUA_ID == fua, \"geometry\"]\n",
    "city = meta.loc[meta.eFUA_ID == fua, \"eFUA_name\"].values[0]\n",
    "\n",
    "gdf_orig = utils.read_original(fua)\n",
    "proj_crs = gdf_orig.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_grid = utils.make_grid(fua, res, proj_crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in results from different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read results from all methods into dict\n",
    "\n",
    "methods = {}\n",
    "\n",
    "for method in methods_to_evaluate:\n",
    "    print(f\"Reading in results for {method}\")\n",
    "    gdf = eval.read_method_outputs(fua, method, proj_crs)\n",
    "    G = mm.gdf_to_nx(gdf, length=\"length\", integer_labels=True)\n",
    "    nodes, edges = mm.nx_to_gdf(G)\n",
    "\n",
    "    # add node degrees\n",
    "    nodes = stats.add_node_degree(nodes, G)\n",
    "\n",
    "    # add stroke IDs\n",
    "    coins = mm.COINS(edges, angle_threshold=120, flow_mode=True)\n",
    "    edges[\"stroke_id\"] = coins.stroke_attribute()\n",
    "    stroke_gdf = coins.stroke_gdf()\n",
    "\n",
    "    methods[method] = {}\n",
    "    methods[method][\"gdf\"] = gdf\n",
    "    methods[method][\"graph\"] = G\n",
    "    methods[method][\"nodes\"] = nodes\n",
    "    methods[method][\"edges\"] = edges\n",
    "\n",
    "    ### grid with stats eval for this method only\n",
    "    grid = base_grid.copy()\n",
    "\n",
    "    # add ratio columns to grid\n",
    "    grid[[\"edge_count\", \"edge_length\"]] = grid.apply(\n",
    "        lambda x: stats.get_edge_stats(edges, x.geometry),  # noqa: B023\n",
    "        axis=1,\n",
    "        result_type=\"expand\",\n",
    "    )\n",
    "    grid[[\"node_count\", \"node_degrees\", \"avg_degree\"]] = grid.apply(\n",
    "        lambda x: stats.get_node_stats(nodes, x.geometry),  # noqa: B023\n",
    "        axis=1,\n",
    "        result_type=\"expand\",\n",
    "    )\n",
    "\n",
    "    grid[[\"stroke_count\", \"stroke_length_sum\", \"stroke_length_max\"]] = grid.apply(\n",
    "        lambda x: stats.get_stroke_stats(edges, stroke_gdf, x.geometry),  # noqa: B023\n",
    "        axis=1,\n",
    "        result_type=\"expand\",\n",
    "    )\n",
    "\n",
    "    # save grid to dict\n",
    "    methods[method][\"grid\"] = grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make weights dict for comparisons\n",
    "\n",
    "weights = {}\n",
    "\n",
    "assert \"manual\" in methods, \"Need to read in manual data first\"\n",
    "assert \"orig\" in methods, \"Need to read in orig data first\"\n",
    "\n",
    "grid = base_grid.copy()\n",
    "\n",
    "# add ratio columns for all evaluation variables\n",
    "for var in eval_vars:\n",
    "    grid[f\"{var}_delta\"] = methods[\"manual\"][\"grid\"][var] - methods[\"orig\"][\"grid\"][var]\n",
    "    grid[grid == np.Inf] = np.NaN\n",
    "\n",
    "# save grid as dict entry\n",
    "weights[\"grid\"] = grid.copy()\n",
    "\n",
    "del grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make evaldict of comparisons\n",
    "\n",
    "evaldict = {}\n",
    "\n",
    "for m1, m2 in methodpairs_to_compare:\n",
    "    # make sure both methods are read into dict\n",
    "    assert m1 in methods, f\"Need to read in {m1} results first!\"\n",
    "    assert m2 in methods, f\"Need to read in {m2} results first!\"\n",
    "\n",
    "    # get base grid\n",
    "    grid = base_grid.copy()\n",
    "\n",
    "    # add ratio columns for all evaluation variables\n",
    "    for var in eval_vars:\n",
    "        grid[f\"{var}_ratio\"] = methods[m1][\"grid\"][var] / methods[m2][\"grid\"][var]\n",
    "        grid[grid == np.Inf] = np.NaN\n",
    "\n",
    "    # save grid as dict entry\n",
    "    evaldict[f\"{m1}_{m2}\"] = grid\n",
    "\n",
    "    del grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "# Usage of evaldict\n",
    "\n",
    "In evaldict, evaluation results are stored.\n",
    "\n",
    "key: pair of methods (left, right); \n",
    "\n",
    "value: grid with `_ratio` columns for evaluation variables (left / right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = \"manual_parenx-voronoi\"\n",
    "var = \"stroke_length_max_ratio\"\n",
    "c1, c2 = comp.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = evaldict[comp].copy()\n",
    "m = cells.explore(tiles=\"cartodb positron\", column=var, cmap=\"Reds\", name=var)\n",
    "methods[c1][\"gdf\"].explore(m=m, name=c1, color=\"black\")\n",
    "methods[c2][\"gdf\"].explore(m=m, name=c2, color=\"green\")\n",
    "folium.LayerControl().add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldict[\"orig_manual\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaldict[\"manual_orig\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = cm.PiYG\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "i = 0\n",
    "evaldict[c1].plot(\n",
    "    ax=ax[i],\n",
    "    column=var,\n",
    "    # norm=colors.CenteredNorm(vcenter=1),\n",
    "    vmin=-3,\n",
    "    vmax=5,\n",
    "    cmap=cmap,\n",
    "    legend=True,\n",
    ")\n",
    "ax[i].set_title(c1)\n",
    "\n",
    "i = 1\n",
    "evaldict[c2].plot(\n",
    "    ax=ax[i],\n",
    "    column=var,\n",
    "    # norm=colors.CenteredNorm(vcenter=1),\n",
    "    vmin=-3,\n",
    "    vmax=5,\n",
    "    cmap=cmap,\n",
    "    legend=True,\n",
    ")\n",
    "ax[i].set_title(c2)\n",
    "\n",
    "plt.suptitle(var);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = \"manual_parenx-skeletonize\"\n",
    "c2 = \"manual_sgeop\"\n",
    "var = \"edge_length_ratio\"\n",
    "\n",
    "cmap = cm.PiYG\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "i = 0\n",
    "evaldict[c1].plot(\n",
    "    ax=ax[i],\n",
    "    column=var,\n",
    "    norm=colors.CenteredNorm(vcenter=1),\n",
    "    cmap=cmap,\n",
    "    legend=True,\n",
    ")\n",
    "ax[i].set_title(c1)\n",
    "\n",
    "i = 1\n",
    "evaldict[c2].plot(\n",
    "    ax=ax[i],\n",
    "    column=var,\n",
    "    norm=colors.CenteredNorm(vcenter=1),\n",
    "    cmap=cmap,\n",
    "    legend=True,\n",
    ")\n",
    "ax[i].set_title(c2)\n",
    "\n",
    "plt.suptitle(var);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simplification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
